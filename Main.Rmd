---
title: "Global CLI Project"
author: "Charles Miele"
date: "2023-02-01"
output:
  html_document:
    toc: yes
    theme: united
    df_print: paged
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(ggplot2)
library(dplyr)
```

# Data Collection
```{r}
# Load data
cli <- read.csv('cost-of-living_v2.csv')

# Remove any columns with NA values
cli <- cli[complete.cases(cli), ]

dim(cli)

varData <- setNames(stack(sapply(cli, class))[2:1], c('variable', 'class'))

descriptions <- read.csv('Descriptions.csv')
descriptions <- descriptions[, c('Column', 'Description')]
gt::gt(descriptions)
```


# Principal-Component Analysis

## Setup
```{r}
# Function for plotting on GG plot. Takes in data and PCs(int) to plot
pca.gg <- function(d, n1, n2) {
  # Variation
  p.var <- d$sdev^2
  p.var.per <- round(p.var / sum(p.var)*100, 1)
  
  ggD <- data.frame(Sample=rownames(d$x), X=d$x[,n1], Y=d$x[,n2])

  ggplot(data=ggD, aes(x=X, y=Y, label=Sample)) +
    geom_text() +
    xlab(paste("PC", n1, p.var.per[n1], "%", sep = " ")) +
    ylab(paste("PC", n2, p.var.per[n2], "%", sep=" ")) +
    ggtitle("PCA Graph")
}

# Loading scores
load_scores <- function(d, n, onCols) {
  loading_scores <- d$rotation[, n]
  col_scores <- abs(loading_scores)
  c_ranked <- sort(col_scores, decreasing = TRUE)
  test <- data.frame(c_ranked[1:10])
  colnames(test) <- "Loading Score"
  test$Column <- row.names(test)
  if (onCols) {
    test <- left_join(test, descriptions, by="Column")
    return(gt::gt(test))
  } else {
    return(gt::gt(test))
  }
}

# Scree plot
s_plot <- function(d){
  pca.var <- d$sdev^2
  pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
  barplot(pca.var.per, main="Screeplot", xlab="Principal Component",
          ylab="% variation")
}
```

## Which cities are most similar?
```{r}
PCA.data <- cli
row.names(PCA.data) <- paste(PCA.data$city, PCA.data$country, sep = ", ")

PCA.data <- subset(PCA.data, select=-c(city, country))
PCA.1 <- prcomp(PCA.data, scale=TRUE, tol = 0.1)
s_plot(PCA.1)

pca.gg(PCA.1, 1, 2)

# Which variables were most influential on where the companies were plotted for PC1 (x-axis?)
load_scores(PCA.1, 1, TRUE)
# Which variables were most influential on where the companies were plotted for PC2 (y-axis?)
load_scores(PCA.1, 2, TRUE)
load_scores(PCA.1, 3, TRUE)
```

## Which metrics are most similar?
```{r}
PCA.data2 <- t(data.matrix(PCA.data))
PCA.data2 <- t(apply(PCA.data2, 1, function(x)(x-min(x))/(max(x)-min(x))))
PCA.2 <- prcomp(PCA.data2)

s_plot(PCA.2)
# Graph
pca.gg(PCA.2, 1, 2)

# What companies were most influential on where the variables were plotted for PC1?
load_scores(PCA.2, 1, FALSE)
# PC2?
load_scores(PCA.2, 2, FALSE)
```

# Lasso Regression
```{r}
y <- cli$x54
x <- cli %>% select(-c(x54, city, country, data_quality))
x <- data.matrix(x)

# Find lambda value
cross_val_model <- cv.glmnet(x, y, alpha = 1)
best_lamda <- cross_val_model$lambda.min
best_lamda

# Produce plot of test MSE by lambda value
plot(cross_val_model)
best_model <- glmnet(x, y, alpha = 1, lamda = best_lamda)
y_pred = predict(best_model, s = best_lamda, newx = x)

# Find SST and SSE
sst <- sum((y- mean(y))^2)
sse <- sum((y_pred - y)^2)

#find R-Squared
rsq <- 1 - sse/sst
# Best model explains {rsq * 100}% of the variation in the response values of the training data
rsq
  


# Which metrics are most important?
# Extracts non-zero coefficients
predictors <- coef(best_model, s = best_lamda)
predictors <- data.matrix(predictors)
predictors <- data.frame(predictors)
predictors$Column <- row.names(predictors)
predictors <- left_join(predictors, descriptions, by="Column")
predictors <- filter(predictors, s1 > 0)
gt::gt(predictors)
```